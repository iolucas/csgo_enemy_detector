{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def load_and_preprocess_image(filename):\n",
    "    filedata = tf.read_file(filename)\n",
    "    image_data = tf.image.decode_png(filedata, channels=3)\n",
    "    image_data = tf.image.crop_to_bounding_box(image_data, 0, 0, 300, 300) #Some images were saved by 301x301, must normalize it\n",
    "    #Return the label values (whether the fifth letter is p of 'pressed')\n",
    "    return image_data, tf.cast(tf.equal(tf.substr(filename,5, 1), 'p'), tf.float32)\n",
    "\n",
    "def mirror_image(image_data, label):\n",
    "    return tf.image.flip_left_right(image_data), label\n",
    "\n",
    "def create_datasets(filenames, batch_size, n_epochs):\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices(filenames)\n",
    "    dataset = dataset.map(load_and_preprocess_image)\n",
    "    mirror_dataset = dataset.map(mirror_image) #Mirror dataset for augment dataset\n",
    "    aug_dataset = dataset.concatenate(mirror_dataset)\n",
    "    aug_dataset = aug_dataset.shuffle(buffer_size=20000, seed=0)\n",
    "    \n",
    "    train_dataset_portion = 0.8\n",
    "    \n",
    "    point_of_split = int(len(filenames) * train_dataset_portion)\n",
    "    \n",
    "    train_dataset = aug_dataset.take(point_of_split).batch(batch_size).repeat(n_epochs)\n",
    "    valid_dataset = aug_dataset.skip(point_of_split).batch(len(filenames) - point_of_split).repeat()\n",
    "    \n",
    "    return train_dataset, valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conv_net(image_batch, reuse=False):\n",
    "    with tf.variable_scope(\"conv_net\", reuse=reuse):\n",
    "        \n",
    "        image_batch /= 255 #Normalize image data\n",
    "        \n",
    "        conv1 = tf.layers.conv2d(image_batch, filters=8, kernel_size=[5,5], strides=[1, 1], padding='SAME',\n",
    "                                kernel_initializer=None, bias_initializer=tf.zeros_initializer(), activation=tf.nn.relu)\n",
    "        conv1 = tf.layers.max_pooling2d(conv1, pool_size=[5,5], strides=[2,2], padding='SAME')\n",
    "        #print(conv1.get_shape())\n",
    "        \n",
    "        conv2 = tf.layers.conv2d(conv1, filters=16, kernel_size=[3,3], strides=[1, 1], padding='SAME',\n",
    "                                kernel_initializer=None, bias_initializer=tf.zeros_initializer(), activation=tf.nn.relu)\n",
    "        conv2 = tf.layers.max_pooling2d(conv2, pool_size=[3,3], strides=[2,2], padding='SAME')\n",
    "        #print(conv2.get_shape())\n",
    "        \n",
    "        conv3 = tf.layers.conv2d(conv2, filters=32, kernel_size=[3,3], strides=[1, 1], padding='SAME',\n",
    "                                kernel_initializer=None, bias_initializer=tf.zeros_initializer(), activation=tf.nn.relu)\n",
    "        conv3 = tf.layers.max_pooling2d(conv3, pool_size=[3,3], strides=[2,2], padding='SAME')\n",
    "        #print(conv3.get_shape())\n",
    "        \n",
    "        conv4 = tf.layers.conv2d(conv3, filters=64, kernel_size=[3,3], strides=[1, 1], padding='SAME',\n",
    "                                kernel_initializer=None, bias_initializer=tf.zeros_initializer(), activation=tf.nn.relu)\n",
    "        conv4 = tf.layers.max_pooling2d(conv4, pool_size=[3,3], strides=[2,2], padding='SAME')\n",
    "        #print(conv4.get_shape())\n",
    "        \n",
    "        conv5 = tf.layers.conv2d(conv4, filters=128, kernel_size=[3,3], strides=[1, 1], padding='SAME',\n",
    "                                kernel_initializer=None, bias_initializer=tf.zeros_initializer(), activation=tf.nn.relu)\n",
    "        conv5 = tf.layers.max_pooling2d(conv5, pool_size=[3,3], strides=[2,2], padding='SAME')\n",
    "        #print(conv5.get_shape())\n",
    "\n",
    "        flatten_layer = tf.layers.flatten(conv5)\n",
    "        \n",
    "        h1 = tf.layers.dense(flatten_layer, 5000, tf.nn.relu)\n",
    "        h2 = tf.layers.dense(h1, 1000, tf.nn.relu)\n",
    "        h3 = tf.layers.dense(h2, 256, tf.nn.relu)\n",
    "        \n",
    "        logits = tf.squeeze(tf.layers.dense(h3, 1), axis=1) \n",
    "        \n",
    "        outputs = tf.nn.sigmoid(logits)\n",
    "        \n",
    "        return logits, outputs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimizer(logits, labels, learning_rate=0.001):\n",
    "    loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "    \n",
    "    return optimizer, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_accuracy(outputs, labels):\n",
    "    \n",
    "    outputs = tf.cast(outputs > 0.5, tf.bool)\n",
    "    labels = tf.cast(labels, tf.bool)\n",
    "    \n",
    "    accuracy, update_op = tf.metrics.accuracy(outputs, labels)\n",
    "    \n",
    "    return accuracy, update_op\n",
    "\n",
    "def reset_accuracy_variables():\n",
    "    return [tf.assign(v,0) for v in tf.local_variables() if 'accuracy' in v.name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing session...\n",
      "Step 100/1000 \t Loss: 0.38526108860969543\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 1\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "filenames = [\"imgs/\" + filename for filename in os.listdir(\"imgs\")][:10000]\n",
    "\n",
    "N_STEPS = math.ceil(len(filenames)* 0.8 * N_EPOCHS/BATCH_SIZE)\n",
    "\n",
    "#-------------------------------------------------#\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "train_dataset, valid_dataset = create_datasets(filenames, BATCH_SIZE, N_EPOCHS)\n",
    "\n",
    "train_dataset_x_batch, train_dataset_y_batch = train_dataset.make_one_shot_iterator().get_next()\n",
    "valid_dataset_x_batch, valid_dataset_y_batch = valid_dataset.make_one_shot_iterator().get_next()\n",
    "\n",
    "logits, outputs = create_conv_net(train_dataset_x_batch)\n",
    "optimizer, loss = create_optimizer(logits, train_dataset_y_batch, 0.001)\n",
    "\n",
    "valid_logits, valid_outputs = create_conv_net(valid_dataset_x_batch, reuse=True)\n",
    "acc, update_acc = create_accuracy(valid_outputs, valid_dataset_y_batch)\n",
    "reset_acc = reset_accuracy_variables()\n",
    "\n",
    "print(\"Initializing session...\")\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    \n",
    "    i_step = 0\n",
    "    \n",
    "    while True:\n",
    "        i_step += 1\n",
    "        \n",
    "        try:\n",
    "            _, loss_value = sess.run([optimizer, loss])\n",
    "            \n",
    "            if i_step % 100 == 0:\n",
    "                print(\"Step {}/{} \\t Loss: {}\".format(i_step, N_STEPS, loss_value))\n",
    "                \n",
    "                sess.run(reset_acc)\n",
    "                acc_value = sess.run(update_acc)\n",
    "                print(\"Validation accuracy: {}\".format(acc_value))\n",
    "\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print(\"End of dataset\")  # ==> \"End of dataset\"\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
